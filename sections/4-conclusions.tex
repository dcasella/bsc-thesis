%!TEX root = ../thesis.tex

\section{Conclusions}\label{section:conclusions}

In this thesis we provided an overview of the Persistent Phylogeny problem, some possible applications of it and we tackled the task of implementing the recently introduced polynomial-time algorithm \cite{PPPptime2016} using the \cc{} language and the Boost Graph Library.

We described the process of implementing the proposed algorithm giving a description of the more critical functions, the implementation choices that have been made during development and the reasoning behind them.

\subsection{Performed experiments}\label{section:experiments}

Our dataset consists of 8030 PPP instances, ranging from $10 \times 10$ matrices up to $1280 \times 2560$ with various levels of mutations; 3996 of these instances were generated to admit a persistent phylogeny.

Instances that admit a persistent phylogeny are named starting with !ok!, otherwise they start with !no!, e.g., an instance !ok_320_320_0.01_0_M.txt! is a $320 \times 320$ instance admitting a persistent phylogeny ($0.01$ represents the mutations; $0$ indicates it's the first $\{ 320 \times 320$, $0.01 \}$ generated instance).

The program we implemented outputs a "Ok" or "No" answer to the existence of a persistent phylogeny tree for each instance in input, e.g., !Ok (tests/input/ok_320_320_0.01_0_M.txt)!.
This can be extended to display the actual c-reductions by launching the program with the verbose option.

Most tests have been performed on instances reduced to having only maximal characters; this implies that the actual size of the randomly generated instances we worked on is drastically limited.
The number of maximal characters in the instances admitting a persistent phylogeny ranges from 1 to 29 (up to 37 in instances that don't admit a PP).

Figure \ref{figure:6} shows the plotted distribution.

\begin{figure}[hp]
  \input{figures/figure6}

  \caption{Plotted distribution for the number of maximal characters in the dataset of PPP instances}\label{figure:6}
\end{figure}

Running the program on the set of 3996 "Ok" instances reduced to maximal produced the following results:

\lstset{style=code_block}

\begin{lstlisting}[belowskip=0pt]
  $\$$ bin/ppp tests/input/ok_* -m
  Running PPP on 3996 files.

  Ok (tests/input/ok_10_10_0.32_8_M.txt)
  Ok (tests/input/ok_10_10_0.04_4_M.txt)
  Ok (tests/input/ok_10_10_0.02_17_M.txt)
  Ok (tests/input/ok_10_10_0.01_13_M.txt)
  Ok (tests/input/ok_10_10_0.08_2_M.txt)
  $\dots$
  Ok (tests/input/ok_1280_2560_0.64_0_M.txt)
  Ok (tests/input/ok_1280_2560_.64_1_M.txt)
  Ok (tests/input/ok_1280_2560_.64_2_M.txt)
  Ok (tests/input/ok_1280_2560_.64_3_M.txt)
  Ok (tests/input/ok_1280_2560_.64_4_M.txt)
\end{lstlisting}

\begin{lstlisting}
  $\$$ bin/ppp tests/input/ok_* -m | grep -c No
  0
\end{lstlisting}

The computation took $773.97s$ to complete.

Moreover, we verified that the algorithm is able to compute a successful c-reduction for each possible combination of safe sources, by running the program on the same set of instances in its exponential version and checking whether the program logged the string "No for safe source \species{}", which indicates that having selected the source \species{} as a safe source was an error.

\begin{lstlisting}
  $\$$ bin/ppp tests/input/ok_* -m -x -v | grep -c "No for safe source"
  0
\end{lstlisting}

Testing the functionality of the program on instances with only a limited set of maximal characters doesn't reflect its use-case, even if it's not complete.

We show that applying the algorithm to a set of 2016 "Ok" instances without selecting the Maximal option produces a reasonable amount of correct results (approximately 90\%).

\begin{lstlisting}
  $\$$ bin/ppp tests/input/ok_??_* | grep -c No
  208
\end{lstlisting}

While adding the Exponential option to the executed command produces the following output:

\begin{lstlisting}
  $\$$ bin/ppp tests/input/ok_??_* -x | grep -c No
  11
\end{lstlisting}

From these results we can see that testing every possible safe source is sure to produce better results; this is because we have yet to implement a subset of conditions to work with minimal characters in the graph.

The algorithm, while being structured to produce a persistent phylogeny tree $T$, fails correctly when the tree $T$ doesn't exist.
We can test this by applying it to the 4031 No instances of the dataset.

\begin{lstlisting}[belowskip=0pt]
  $\$$ bin/ppp tests/input/no_* | grep -c Ok
  0
\end{lstlisting}

\subsection{Implementation improvements}\label{section:impl-improvements}

The current implementation of signed characters and Hasse diagram sees the use of vertex names as the parameter that can uniquely identify a vertex in a series of red-black graphs (\grb{} and \gm{}).
Recently, we came across the idea of assigning to each vertex an index which, in the initial graph \grb{}, would represent the possible position of each vertex in a vector.
This way, we can combine the stabiity of !boost::slistS! as the underlying structure of the vertices, while being able to address directly and in constant-time each vertex across graph and vertex copies.
This change would make it possible to use a simple !std::map! mapping vertex indexes to vertex descriptors in each graph (instead of the bidirectional map) and it would simplify a multitude of functions by not having to work with specifically constructed strings.

The correctness of the computation of the chains in a Hasse diagram must be certain.
As described in section \ref{section:initial-state}, this is not quite confirmed.
A simple DFS visit may not be the most optimal solution \textendash{} while being a reasonable choice of algorithm.
Implementing an altered version of the DFS visit, with its focus being finding the chains starting from a fixed source, could solve the problems we have with the current state of the implementation.

The script !check_reduction.py! we used to verify the output of the \cc{} program proved to run way faster on Python2.7 than on Python3.
We haven't been able to track down the issue yet, but it seemed like NumPy took an incommensurate amount of time to generate the ndarray from the input file.
We will try to update the script to support Python3 without performance losses.

\subsubsection{Bottlenecks}\label{section:bottlenecks}

Profiling our application with Valgrind (Callgrind tool) showed a substantial amount of calls to the !is_free! and !is_universal! functions, mainly generated by the realization of signed characters in the graph.
This is to be expected, however, every time the algorithm calls one of the aforementioned functions, it computes the connected components of the graph.
A possible approach to solve this is by calculating the connected components of the graph once and passing the result by reference to the functions that search for free and universal characters.

When verifying whether a realization is feasible and if it induces red \sg{}s, we create a copy of the current graph \grb{} (or \gm{}) and apply the c-reduction to it.
This is perfectly fine, however, in some cases it might be possible to substitute or mirror the reduced graph (which is the copy) to the actual graph, e.g., the realization of a safe source.

\subsection{Further development}\label{section:further-dev}

We introduced the algorithm (\ref{algorithm:find-initial-state}) for computing an initial state of a non degenerate tree $T$ solving \grb{}.
However, in section \ref{section:safe-chains-sources}, we explain that in its current form the algorithm doesn't support the presence of minimal characters in the original graph \grb{}.
We plan to extend our implementation by adding support for minimal characters, consequently introducing conditions that will either further limit some of the checks already in place, or extend the algorithm in a non restraining way.
